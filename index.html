<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<!-- <html> -->
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
    <title>Hanshu Yan (严汉书)</title>
    <!-- <base href="https://hanshuyan.github.io"> -->
    <!-- <base href="./index"> -->
</head>

<body style="margin-left:200; margin-right:200; padding:0">

    <div id="toptitle">
        <h1 style="padding-left: 0" lang="zh" >Hanshu YAN (严 汉书) <font size="-0"> (Pronouns: he/him/his)</font></h1>
    </div>
    <!-- <div w3-include-html="b.html"></div> -->


<table summary="Table for page layout." id="tlayout" class="center">
    <tr valign="top">
    <!-- <td id="layout-menu" style="width:10%"> -->
        <!-- <img src="hanshu.png" style="max-width:100%;height:auto" alt="Hanshu YAN" /> <br> <br> -->
        <!-- <i class="material-icons">school</i>&nbsp;<a href="https://scholar.google.com.sg/citations?user=xRz9vN4AAAAJ&hl=en">[Scholar]</a> <br> -->
        <!-- &nbsp;<i class="fa fa-linkedin-square fa-lg"></i>&nbsp;<a href="https://www.linkedin.com/in/hanshu-yan-786845126/?locale=de_DE">[Linkedin]</a> <br> -->
    <!-- </td> -->

    <td id="layout-content" style="width:95%">
        <table class="imgtable">
        <tbody>
        <tr>
        <td>
            <img class="img-responsive" onMouseOver="this.src='assets/hanshu2.png';" onMouseOut="this.src='assets/hanshu2.png';" src="assets/hanshu2.png" width = "auto" height = "200px" /> &nbsp;&nbsp;
        </td>
        <td align="left">
            <p>Senior Research Scientist @ Rhymes.AI <br></p>
            <p><i class="fa fa-envelope-o" aria-hidden="true"></i>:&nbsp;hanshu.yan@outlook.com <br></p>
            <p>
                <i class="material-icons">school</i>&nbsp;[<a href="https://scholar.google.com/citations?user=MG817V4AAAAJ&hl=zh-CN&authuser=1">Scholar</a>] &nbsp;
                <i class="fa fa-linkedin-square fa-lg"></i>&nbsp;[<a href="https://www.linkedin.com/in/hanshu-yan-786845126/?locale=de_DE" >Linkedin</a>] 
            </p>
            <p><br></p>
            <p>We are actively looking for talented research interns working on multimodal understanding and video generation/restoration.</p>
            
        </td>
        </tr>
         </tbody> 
        </table>

        <!-- Biography -->
        <div id="bio", style="margin:30 auto;" >
            <h2><a name="biography"></a>About Me</h2>
            <p>
                He obtained PhD from NUS in 2022 with a research focus on robust deep vision models and optimizers. 
                His current work focuses on multimodal generation and understanding. 
                He has led or co-led several relevant projects, including <a href="https://github.com/magic-research/magic-edit">MagicEdit</a> and <a href="https://github.com/magic-research/magic-avatar">MagicAvatar</a>  for video editing, 
                <a href="https://github.com/magic-research/piecewise-rectified-flow">PeRFlow</a> for diffusion acceleration,
                and <a href="https://github.com/HanshuYAN/AdjointDPM">AdjointDPM</a> for controllable generation.
                He was also involved in several large-scale video generation projects, including MagicVideo and <a href="https://github.com/rhymes-ai/Allegro">Allegro</a>. 
                He will be dedicated to developing efficient and powerful models for video generation and understanding.
            </p>

            Academic Service <br>
            <ul>
                <li>Reviewer for ICML, ICLR, NeurIPS, CVPR, ACM Multimedia, etc.</li>
            </ul>

            <b>Education</b>
            <ul style="list-style-type: none">
                <table style="width: 80%">
                    <tr>
                        <td style="width:40%">Ph.D,&nbsp; Deep Learning</td> <td style="width:40%"> NUS </td> <td style="width:20%">2022</td>
                    </tr>
                    <tr>
                        <td>M.Sc,&nbsp; EE</td> <td> NUS </td> <td>2017</td>
                    </tr>
                    <tr>
                        <td>B.Eng, EE & Math </td> <td> BUAA </td> <td>2015</td>
                    </tr>
                    </table>
            </ul>

        </div>




        <!-- Publications -->
        <div id="papers"; style="margin:30 auto;" >
            <h2><a name="publications"></a>Selected Papers</h2> 
            (
            <i>
                * Equally contributed; 
                <mark class='c0'>^</mark>AIGC, 
                <mark class='c1'>^</mark>ML robustness, 
                <mark class='c2'>^</mark>Learning algorithms, 
                <mark class='c3'>^</mark>Others
            </i>
            )

            <ul style="list-style-type: none">
                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2405.07510">PeRFlow; Piecewise Rectified Flow as Universal Plug-and-Play Accelerator</a>. <i>NeurIPS 2024</i> <br>
                    Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, Jiashi Feng
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2405.13722">InstaDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos</a>. <i>arXiv 2024</i> <br>
                    Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent Y. F. Tan, Jiashi Feng
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2306.14435">DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</a>. <i>CVPR 2024</i> <br>
                    Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2401.04468">MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</a>. <i>arXiv 2024</i> <br>
                    Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng
                </li><br>
                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2211.11018">MagicVideo: Efficient Video Generation With Latent Diffusion Models</a>. <i>arXiv 2023</i> <br>
                    Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2311.16498">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model
                    </a>. <i>arXiv 2023</i> <br>
                    Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, Mike Zheng Shou
                </li><br>
                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2308.14748">MagicAvatar: Multimodal Avatar Generation and Animation</a>. <i>arXiv 2023</i> <br>
                    Jianfeng Zhang*, Hanshu Yan*, Zhongcong Xu*, Jiashi Feng, Jun Hao Liew*
                </li><br>
                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2308.14749">MagicEdit: High-Fidelity and Temporally Coherent Video Editing</a>. <i>arXiv 2023</i> <br>
                    Jun Hao Liew*, Hanshu Yan*, Jianfeng Zhang, Zhongcong Xu, Jiashi Feng
                </li><br>
                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2309.00908">MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation</a>. <i>arXiv 2023</i> <br>
                    Hanshu Yan*, Jun Hao Liew*, Long Mai, Shanchuan Lin, Jiashi Feng
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2312.12030">SAG: Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method</a>. <i>arXiv 2023</i> <br>
                    Jiachun Pan*, Hanshu Yan*, Jun Hao Liew, Jiashi Feng, Vincent Y. F. Tan
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2307.10711">AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</a>. <i>ICLR 2024</i> <br>
                    Jiachun Pan*, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan*
                </li><br>

                <li>
                    <mark class='c0'>^</mark> <a href="https://arxiv.org/abs/2210.16056">MagicMix: Semantic Mixing with Diffusion Models</a>. <i>arXiv 2022</i> <br>
                    Jun Hao Liew*, Hanshu Yan*, Daquan Zhou, Jiashi Feng
                </li><br>

                <!-- Robustness -->
                <li>
                    <mark class='c1'>^</mark> <a href="https://www.sciencedirect.com/science/article/pii/S0893608023005890">Learning a robust foundation model against clean-label data poisoning attacks at downstream tasks</a>. <i>Neural Networks 2023</i> <br>
                    Ting Zhou, Hanshu Yan*, Bo Han, Lei Liu, Jingfeng Zhang
                </li><br>
                <li>
                    <mark class='c1'>^</mark> <a href="">Pre-training Robust Feature Extractor Against Clean-label Data Poisoning Attacks</a>. <i>NeurIPS Workshop on Machine Learning Safety 2022</i> <br>
                    Ting Zhou, Hanshu Yan*, Lei Liu, Jingfeng Zhang, Bo Han
                </li><br>
                <li>
                    <mark class='c1'>^</mark> <a href="https://arxiv.org/abs/2201.04397">Towards Adversarially Robust Deep Image Denoising</a>. <i>IJCAI 2022</i> <br>
                    Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan 
                </li><br>

                <li>
                    <mark class='c1'>^</mark> <a href="http://proceedings.mlr.press/v139/yan21e.html">CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection</a>. <i>ICML 2021</i> 
                    &nbsp;[<a href="https://github.com/HanshuYAN/CIFS">Code</a>]
                    &nbsp;[<a href="https://icml.cc/virtual/2021/spotlight/10648">Poster</a>] <br>
                    Hanshu Yan, Jingfeng Zhang, Gang Niu, Jiashi Feng, Vincent Y. F. Tan, Masashi Sugiyama
                </li><br>

                <li>
                    <mark class='c1'>^</mark> <a href="https://arxiv.org/pdf/2004.14798.pdf">RAIN: Robust and Accurate Classification Networks with Randomization and Enhancement</a>. <i>arXiv 2020</i> 
                    &nbsp;[<a href="https://github.com/dydjw9/RAIN">Code</a>] <br>
                    Jiawei Du, Hanshu Yan, Joey Tianyi Zhou, Rick Siow Mong Goh, Jiashi Feng
                </li><br>

                <li>
                    <mark class='c1'>^</mark> <a href="https://arxiv.org/abs/1910.05513">On Robustness of Neural Ordinary Differential Equations</a>. <i>ICLR 2020 Spotlight</i> 
                    &nbsp;[<a href="https://github.com/HanshuYAN/TisODE">Code</a>] <br>
                    Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, Jiashi Feng
                </li><br>

                <!-- Learning algorithms -->
                <li>
                    <mark class='c2'>^</mark> <a href="https://pubs.acs.org/doi/abs/10.1021/acs.analchem.1c01559">Towards Better Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach</a>. <i>ICLR 2024</i>  <br>
                    Xiang Lan, Hanshu Yan, Shenda Hong, Mengling Feng
                </li><br>
                <li>
                    <mark class='c2'>^</mark> <a href="https://arxiv.org/abs/2110.03141">Efficient Sharpness-aware Minimization for Improved Training of Neural Networks</a> . <i>ICLR 2022</i> 
                    &nbsp;[<a href="https://github.com/dydjw9/Efficient_SAM">Code</a>] <br>
                    Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, Vincent Y. F. Tan
                </li><br>

                <li>
                    <mark class='c2'>^</mark> <a href="https://papers.nips.cc/paper/2021/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html">Towards Understanding Why Lookahead Generalizes Better Than SGD and Beyond</a>. <i>NeurIPS 2021</i> <br>
                    Pan Zhou, Hanshu Yan, Xiaotong Yuan, Jiashi Feng, Shuicheng Yan 
                </li><br>

                <li>
                    <mark class='c2'>^</mark> <a href="https://arxiv.org/abs/2110.00926">Information-Theoretic Characterization of the Generalization Error for Iterative Semi- Supervised Learning</a>. <i>Journal of Machine Learning Research, 2022.</i>  <br>
                    Haiyun He, Hanshu Yan, Vincent Y. F. Tan
                </li><br>



                <!-- Others -->

                <li>
                    <mark class='c3'>^</mark> <a href="https://pubs.acs.org/doi/abs/10.1021/acs.analchem.1c01559">Deep learning-guided fiberoptic Raman spectroscopy enables real-time in vivo diagnosis and assessment of nasopharyngeal carcinoma</a>. <i>Analytical Chemistry 2021</i>  <br>
                    Chi Shu, Hanshu Yan, Wei Zheng, Kan Lin, Anne James, Selvarajan, Sathiyamoorth, Chwee Ming Lim,  Zhiwei Huang
                </li><br>


                <li>
                    <mark class='c3'>^</mark> <a href="https://arxiv.org/abs/2107.02112">Recovering the Unbiased Scene Graphs from the Biased Ones</a>. <i>ACM MM 2021</i> 
                    &nbsp;[<a href="https://github.com/coldmanck/recovering-unbiased-scene-graphs">Code</a>]<br>
                    Meng-Jiun Chiou, Henghui Ding, Hanshu Yan, Changhu Wang, Roger Zimmermann, Jiashi Feng
                </li><br>


                <li>
                    <mark class='c3'>^</mark> <a href="https://www.ijcai.org/proceedings/2021/0207.pdf">Modeling Trajectories with Neural Ordinary Differential Equations</a>. <i>IJCAI 2021</i> <br>
                    Yuxuan Liang, Kun Ouyang, Hanshu Yan, Yiwei Wang, Zekun Tong and Roger Zimmermann 
                </li><br>


                <li>
                    <mark class='c3'>^</mark> <a href="">Adversarial Domain Adaptation with Prototype-Based Normalized Output Conditioner</a>. <i>IEEE Transactions on Image Processing 2021</i> <br>
                    Dapeng Hu, Jian Liang, Qibin Hou, Hanshu Yan, Yunpeng Chen
                </li><br>
                
                
            </ul>
        </div>



        <div style="margin:30 auto;" >
            <h2><a name="service"></a>Experiences</h2>
            <ul style="list-style-type: none">
                <table style="width:80%">
                    <tr>
                        <td style="width:35%">Senior Research Scientist</td> <td style="width:35%">Rhymes.AI, Singapore</td> <td style="width:30%">07/2024 - Present</td>
                    </tr>
                    <tr>
                        <td>Research Scientist</td> <td>ByteDance, Singapore</td> <td>07/2022 - 07/2024</td>
                    </tr>
                    <tr>
                        <td>Teaching Assistant</td> <td>NUS, Singapore</td> <td>08/2018 - 12/2021</td>
                    </tr>
                    <tr>
                        <td>Reserach Engineer</td> <td>NUS, Singapore</td> <td>07/2017 - 07/2018</td>
                    </tr>
                    </table>
            </ul>

        </div>


        <!-- <div style="margin:30 auto;" >
            <h2><a name="service"></a>Scholarships & Awards</h2>
            <ul>
                <li>NUS Research Scholarship, 2018-2022</li>

                <li>Outstanding Graduating Student (B.Eng @ BUAA), 2015</li>
            </ul>
        </div> -->

        <!-- <div style="margin:30 auto;" >
            <h2><a name="service"></a>Coursework</h2>
            @ School
            <ul>
                <li>EE5907 Pattern Recognition; EE4603 Biomedical Imaging Systems</li>

                <li>EE5904R Neural Networks; EE6934 Deep Learning</li>

                <li>EE5137 Stochastic Processes; EE5139 Information Theory</li>

                <li>EE5101R Linear Systems; EE5103R Computer Control Systems</li>

                <li>EE5702R Advanced Power System Analysis</li>
            </ul>

            @ Home
                <ul>
                    <li>NUS, MA3209 Metric space</li>

                    <li>CMU, 36-705 Intermediate Statistics</li>

                    <li>Stanford, CS224n Natural Language Processing with Deep Learning</li>  
                </ul>
            <a href="googleae4cabe185965a36.html"><font size="6"face="arial" color="#FFFFFF">[tmp!]</font></a>
        </div> -->


    </td>

    <!-- For the purpose of adjusting margin -->
    <td id="layout-content" style="width:20%"></td>
    </tr>


</table>
</body>





</html>
